{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Robatweets_github.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_3kF5Bdn3Ao1",
        "A1VZ5lebPoTd"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6w4BDQeLca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bd56714-1df8-4380-db43-a004e719a4fe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32zs2toRNBCz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f741474-8174-40e8-e33f-40b618d640f0"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "!pip install GetOldTweets3\n",
        "\n",
        "#paquetes y librerias necesarias\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import tqdm\n",
        "import time\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from IPython.display import clear_output\n",
        "from dateutil.parser import parse\n",
        "from datetime import datetime\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "import GetOldTweets3 as got\n",
        "import datetime\n",
        "from textblob import TextBlob\n",
        "from IPython.display import clear_output\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "\n",
        "#from datetime import datetime\n",
        "#from dateutil.parser import parse\n",
        "#from googletrans import Translator\n",
        "\n",
        "\n",
        "%matplotlib inline "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "Collecting GetOldTweets3\n",
            "  Downloading GetOldTweets3-0.0.11-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from GetOldTweets3) (4.2.6)\n",
            "Collecting pyquery>=1.2.10\n",
            "  Downloading pyquery-1.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting cssselect>0.7.9\n",
            "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: cssselect, pyquery, GetOldTweets3\n",
            "Successfully installed GetOldTweets3-0.0.11 cssselect-1.1.0 pyquery-1.4.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzmlwiZHO7Kh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3kF5Bdn3Ao1"
      },
      "source": [
        "## Creacion del df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwcpO0XoNCC_"
      },
      "source": [
        "def listmaker(username,n):\n",
        "    lista = [username] * n\n",
        "    return lista\n",
        "\n",
        "#Funcion para la obtencion de los tweets\n",
        "def robatweets(username,fecha_ini,fecha_fin):\n",
        "\n",
        "  tweetCriteria = got.manager.TweetCriteria().setUsername(username).setSince(fecha_ini).setUntil(fecha_fin)\n",
        "\n",
        "  tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
        "\n",
        "  fechas = [datetime.date(tweet.date.year,tweet.date.month,tweet.date.day).isoformat() for tweet in tweets]\n",
        "  todos_tweets = [tweet.text for tweet in tweets]\n",
        "  nombre = listmaker(username = username,n=len(fechas))\n",
        "  return [todos_tweets,fechas,nombre]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAcERT_yXWHT"
      },
      "source": [
        "\n",
        "fecha_ini = \"2016-01-01\"\n",
        "fecha_fin = \"2020-06-06\"\n",
        "\n",
        "#listas VOX\n",
        "#Santi_ABASCAL = robatweets(username = '@Santi_ABASCAL',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#Jorgebuxade = robatweets(username ='@Jorgebuxade',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
        "#vicpiedra = robatweets(username = '@vicpiedra',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
        "#rromerovilches = robatweets(username = '@rromerovilches',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
        "#Pablosaenzd = robatweets(username = '@Pablosaenzd',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
        "#Vox_Molina = robatweets(username = '@Vox_Molina',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
        "#MeerRocio = robatweets(username = '@MeerRocio',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
        "#Igarrigavaz = robatweets(username = '@Igarrigavaz',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
        "#jlsteeg = robatweets(username = '@jlsteeg',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
        "#RuizSolas = robatweets(username = '@RuizSolas',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
        "#pedro_fhz = robatweets(username = '@pedro_fhz',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "\n",
        "#listas Podemos\n",
        "#PabloIglesias = robatweets(username = '@PabloIglesias',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#MiguelUrban = robatweets(username = '@MiguelUrban',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#pilar_lima = robatweets(username = '@pilar_lima',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#anamarcelloana = robatweets(username = '@anamarcelloana',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#pbustinduy = robatweets(username = '@pbustinduy',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#Alber_Canarias = robatweets(username = '@Alber_Canarias',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#ionebelarra = robatweets(username = '@ionebelarra',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#MayoralRafa = robatweets(username = '@MayoralRafa',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#Julio_Rodr_ = robatweets(username = '@Julio_Rodr_',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#VeraNoelia = robatweets(username = '@VeraNoelia',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#IreneMontero = robatweets(username = '@IreneMontero',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#pnique = robatweets(username = '@pnique',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#VicencNavarro = robatweets(username = '@VicencNavarro',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "\n",
        "#listas PSOE\n",
        "#sanchezcastejon = robatweets(username = '@sanchezcastejon',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#CristinaNarbona = robatweets(username = '@CristinaNarbona',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#Adrilastra = robatweets(username = '@Adrilastra',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#abalosmeco = robatweets(username = '@abalosmeco',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#santicl = robatweets(username = '@santicl',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#JoseantonioJun = robatweets(username = '@JoseantonioJun',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#patxilopez = robatweets(username = '@patxilopez',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#oscar_puente_ = robatweets(username = '@oscar_puente_',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#gomezdcelis = robatweets(username = '@gomezdcelis',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "\n",
        "#listas PP\n",
        "#pablocasado_ = robatweets(username = '@pablocasado_',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#JavierMaroto = robatweets(username = '@JavierMaroto',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
        "#ja_nietob = robatweets(username = '@ja_nietob',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#AlmeidaPP_ = robatweets(username = '@AlmeidaPP_',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#sanchezcesar = robatweets(username = '@sanchezcesar',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#abeltran_ana = robatweets(username = '@abeltran_ana',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#javiermarquezsa = robatweets(username = '@javiermarquezsa',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#vicentetiradopp = robatweets(username = '@vicentetiradopp',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#DiegoCalvoPouso = robatweets(username = '@DiegoCalvoPouso',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
        "\n",
        "#listas Ciudadanos\n",
        "#InesArrimadas = robatweets(username = '@InesArrimadas',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#MarinaBS_Cs = robatweets(username = '@MarinaBS_Cs',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
        "#CCuadradoCs = robatweets(username = '@CCuadradoCs',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#jmespejosaav = robatweets(username = '@jmespejosaav',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#joanmesquida62 = robatweets(username = '@joanmesquida62',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#MelisaRguezCs = robatweets(username = '@MelisaRguezCs',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#BalEdmundo = robatweets(username = '@BalEdmundo',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#Tonicanto1 = robatweets(username = '@Tonicanto1',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
        "#Lroldansu = robatweets(username = '@Lroldansu',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
        "#Albert_Rivera = robatweets(username = '@Albert_Rivera',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
        "#ignacioaguado = robatweets(username = '@ignacioaguado',fecha_ini = fecha_ini, fecha_fin = fecha_fin)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I7pMQk0Xjwy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "778023d5-5243-4af2-9f9c-7daf7748eeff"
      },
      "source": [
        "#creamos las listas completas de los partidos, las cuales incluyen el texto, la fecha y el autor\n",
        "\n",
        "i=0\n",
        "VOX = []\n",
        "Podemos = []\n",
        "PSOE = []\n",
        "PP = []\n",
        "Ciudadanos = []\n",
        "\n",
        "for i in range(3):\n",
        "    VOX.append(Santi_ABASCAL[i] + Jorgebuxade[i] + vicpiedra[i] + rromerovilches[i] + Pablosaenzd[i] + Vox_Molina[i] + MeerRocio[i] + Igarrigavaz[i] + jlsteeg[i]  + RuizSolas[i]  + pedro_fhz[i]) \n",
        "    Podemos.append(PabloIglesias[i] + MiguelUrban[i] + pilar_lima[i] + anamarcelloana[i] + pbustinduy[i] + Alber_Canarias[i] + ionebelarra[i] + MayoralRafa[i] + Julio_Rodr_[i] + VeraNoelia[i] + IreneMontero[i] + pnique[i] + VicencNavarro[i]) \n",
        "    PSOE.append(sanchezcastejon[i] + CristinaNarbona[i] + Adrilastra[i] + abalosmeco[i] + santicl[i] + JoseantonioJun[i] + patxilopez[i] + oscar_puente_[i] + gomezdcelis[i])\n",
        "    PP.append(pablocasado_[i] + JavierMaroto[i] + ja_nietob[i] + AlmeidaPP_[i] + sanchezcesar[i] + abeltran_ana[i] + javiermarquezsa[i] + vicentetiradopp[i] + DiegoCalvoPouso[i])\n",
        "    Ciudadanos.append(InesArrimadas[i] + MarinaBS_Cs[i] + CCuadradoCs[i] + jmespejosaav[i] + joanmesquida62[i] + MelisaRguezCs[i] + BalEdmundo[i] + Tonicanto1[i] + Lroldansu[i] + Albert_Rivera[i] + ignacioaguado[i]) \n",
        "\n",
        "print(len(VOX[0]))\n",
        "print(len(Podemos[0]))\n",
        "print(len(PSOE[0]))\n",
        "print(len(PP[0]))\n",
        "print(len(Ciudadanos[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33508\n",
            "37899\n",
            "31253\n",
            "22970\n",
            "31453\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-g_Gzf7zBVl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "c3c37c9b-fff3-48cf-f227-f3ad3bb1db21"
      },
      "source": [
        "VOX_complete_list = []\n",
        "Podemos_complete_list = []\n",
        "PSOE_complete_list = []\n",
        "PP_complete_list = []\n",
        "Ciudadanos_complete_list = []\n",
        "\n",
        "#preparamos para el CSV\n",
        "#Filtramos de manera que solo se queden los tweets con mas de 7 palabras. Por otro lado añadimos la formacion politica y su posición en el periodo estudiado: oposicion y gobierno\n",
        "for i in range(len(VOX[0])):\n",
        "  if len(VOX[0][i].split()) > 7:\n",
        "    VOX_complete_list.append([VOX[0][i],'VOX','OPO',VOX[2][i],VOX[1][i]])\n",
        "for i in range(len(Podemos[0])):\n",
        "  if len(Podemos[0][i].split()) > 7:\n",
        "    Podemos_complete_list.append([Podemos[0][i],'POD','GOV',Podemos[2][i],Podemos[1][i]])\n",
        "for i in range(len(PSOE[0])):\n",
        "  if len(PSOE[0][i].split()) > 7:\n",
        "    PSOE_complete_list.append([PSOE[0][i],'PSOE','GOV',PSOE[2][i],PSOE[1][i]])\n",
        "for i in range(len(PP[0])):\n",
        "  if len(PP[0][i].split()) > 7:\n",
        "    PP_complete_list.append([PP[0][i],'PP','OPO',PP[2][i],PP[1][i]]) \n",
        "for i in range(len(Ciudadanos[0])):\n",
        "  if len(Ciudadanos[0][i].split()) > 7:\n",
        "    Ciudadanos_complete_list.append([Ciudadanos[0][i],'CIU','OPO',Ciudadanos[2][i],Ciudadanos[1][i]])   \n",
        "\n",
        "\n",
        "lista_completa = VOX_complete_list + Podemos_complete_list + PSOE_complete_list + PP_complete_list + Ciudadanos_complete_list\n",
        "import random\n",
        "lista_completa_mezclada=random.sample(lista_completa,k=len(lista_completa))\n",
        "\n",
        "print(len(lista_completa))\n",
        "print(len(lista_completa_mezclada))\n",
        "\n",
        "lista_completa_mezclada[4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "138203\n",
            "138203\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['#ComienzaElCambio en Andalucía tras cuatro décadas de gobiernos socialistas con @JuanMa_Moreno, nuevo presidente de la Junta. Es un honor ser Presidente del @PPopular en este momento histórico para España. #InvestiduraAnd #JuanmaPresidente',\n",
              " 'PP',\n",
              " 'OPO',\n",
              " '@pablocasado_',\n",
              " '2019-01-16']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 337
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAFVnb6DgWDt"
      },
      "source": [
        "import csv\n",
        "with open('/content/gdrive/My Drive/proyecto_machine_learning/tweets_definitivo_hasta_2016_version_.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"numero\", \"tweet\", \"nombre\",\"posicion\", \"cuenta\", \"fecha\"])\n",
        "    for i in range(len(lista_completa_mezclada)):\n",
        "        writer.writerow([i, lista_completa_mezclada[i][0], lista_completa_mezclada[i][1], lista_completa_mezclada[i][2],lista_completa_mezclada[i][3],lista_completa_mezclada[i][4]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1VZ5lebPoTd"
      },
      "source": [
        "## Simplificación Versión 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KTXyqBTPb7O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "675fb823-0c29-4de6-8855-4e1eda0f8091"
      },
      "source": [
        "df = pd.read_csv('/content/gdrive/My Drive/proyecto_machine_learning/tweets_definitivo_hasta_2016_version_sin_nada_reducido.csv')\n",
        "df.head(5) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>numero</th>\n",
              "      <th>tweet</th>\n",
              "      <th>nombre</th>\n",
              "      <th>posicion</th>\n",
              "      <th>cuenta</th>\n",
              "      <th>fecha</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Acabo de sumarme a la #HoradelPlaneta con @WWF...</td>\n",
              "      <td>PSOE</td>\n",
              "      <td>GOV</td>\n",
              "      <td>@JoseantonioJun</td>\n",
              "      <td>2017-03-22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>\"Nos hemos quedado huérfanos de una fiscalía q...</td>\n",
              "      <td>POD</td>\n",
              "      <td>GOV</td>\n",
              "      <td>@anamarcelloana</td>\n",
              "      <td>2016-03-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Acto Electoral en las #TorresdeCotillas Jueve...</td>\n",
              "      <td>VOX</td>\n",
              "      <td>OPO</td>\n",
              "      <td>@Vox_Molina</td>\n",
              "      <td>2019-11-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Para llevarse tan mal se entienden muy bien. P...</td>\n",
              "      <td>PSOE</td>\n",
              "      <td>GOV</td>\n",
              "      <td>@sanchezcastejon</td>\n",
              "      <td>2018-03-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>#ComienzaElCambio en Andalucía tras cuatro déc...</td>\n",
              "      <td>PP</td>\n",
              "      <td>OPO</td>\n",
              "      <td>@pablocasado_</td>\n",
              "      <td>2019-01-16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   numero  ...       fecha\n",
              "0       0  ...  2017-03-22\n",
              "1       1  ...  2016-03-04\n",
              "2       2  ...  2019-11-05\n",
              "3       3  ...  2018-03-26\n",
              "4       4  ...  2019-01-16\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 339
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFwTdDFeAKD6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3126d8e9-71dc-45b5-ce8e-e5a8327c9462"
      },
      "source": [
        "# Creamos la funcion para el procesado de datos\n",
        "def text_process(tex):\n",
        "    # 1. Eliminamos todas las cosas de marcacion: puntos, comas, etc\n",
        "    nopunct=[char for char in tex if char not in string.punctuation]\n",
        "    nopunct=''.join(nopunct)\n",
        "    \n",
        "    # 2. Lemmatisation: proceso de agrupacion de palabras segun la raiz. Por ejemplo\n",
        "    #    studing y study en el fondo son la misma palabra\n",
        "    \n",
        "    a=''\n",
        "    i=0\n",
        "    for i in range(len(nopunct.split())):\n",
        "        b=lemmatiser.lemmatize(nopunct.split()[i], pos=\"v\")\n",
        "        a=a+b+' '\n",
        "        \n",
        "    # 3. Eliminamos todas las palabras cortas e inutiles: a, an, the... que no nos\n",
        "    #   dicen absolutamente nada de la forma de escribir del autor (obviamente utilizamos el diccionario español)\n",
        "    return [word for word in a.split() if word.lower() not in stopwords.words('spanish')]\n",
        "    #return [word for word in nopunct.split() if word.lower() not in stopwords.words('spanish')]\n",
        "\n",
        "\n",
        "\n",
        "#df['tweet_limpios'] = df['tweet'].str.replace(\"[^a-zA-Z#áéíóúñ]\", \" \")\n",
        "#df['tweet_limpios'] = df['tweet_limpios'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "#df['tweet_limpios'] = text_process(df['tweet_limpios'])\n",
        "\n",
        "df['tweet_limpios'] = df['tweet'] #creamos una nueva columna en el df\n",
        "\n",
        "for i in tqdm.tqdm(range(len(df['tweet_limpios']))): #Ejecutamos el procesado de datos para cada tweet del df\n",
        "#for i in tqdm.tqdm(range(100)):\n",
        "    df['tweet_limpios'][i] = text_process(df['tweet_limpios'][i])\n",
        "    df['tweet_limpios'][i] = ' '.join(df['tweet_limpios'][i])\n",
        "    time.sleep(0.01) #Esto es para marcar el tiempo, que la funcion es lenta\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/138203 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  0%|          | 4/138203 [00:00<1:01:09, 37.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   numero  ...       fecha\n",
            "0       0  ...  2017-03-22\n",
            "1       1  ...  2016-03-04\n",
            "2       2  ...  2019-11-05\n",
            "3       3  ...  2018-03-26\n",
            "4       4  ...  2019-01-16\n",
            "\n",
            "[5 rows x 6 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 138203/138203 [47:14<00:00, 48.76it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEpmexBxAKS_"
      },
      "source": [
        "#Finalmente añadimos año y mes a nuestro df\n",
        "\n",
        "from dateutil.parser import parse\n",
        "\n",
        "año = []\n",
        "mes = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    año.append(parse(df['fecha'][i]).year)\n",
        "    mes.append(parse(df['fecha'][i]).month)\n",
        "df['año'] = año\n",
        "df['mes'] = mes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbIsoXUsXdoL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "021f5832-fbf6-4959-d7e4-f7ff8d961220"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>numero</th>\n",
              "      <th>tweet</th>\n",
              "      <th>nombre</th>\n",
              "      <th>posicion</th>\n",
              "      <th>cuenta</th>\n",
              "      <th>fecha</th>\n",
              "      <th>tweet_limpios</th>\n",
              "      <th>año</th>\n",
              "      <th>mes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Acabo de sumarme a la #HoradelPlaneta con @WWF...</td>\n",
              "      <td>PSOE</td>\n",
              "      <td>GOV</td>\n",
              "      <td>@JoseantonioJun</td>\n",
              "      <td>2017-03-22</td>\n",
              "      <td>Acabo sumarme HoradelPlaneta WWFespana ¿y http...</td>\n",
              "      <td>2017</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>\"Nos hemos quedado huérfanos de una fiscalía q...</td>\n",
              "      <td>POD</td>\n",
              "      <td>GOV</td>\n",
              "      <td>@anamarcelloana</td>\n",
              "      <td>2016-03-04</td>\n",
              "      <td>quedado huérfanos fiscalía defendido inter tod...</td>\n",
              "      <td>2016</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Acto Electoral en las #TorresdeCotillas Jueve...</td>\n",
              "      <td>VOX</td>\n",
              "      <td>OPO</td>\n",
              "      <td>@Vox_Molina</td>\n",
              "      <td>2019-11-05</td>\n",
              "      <td>Acto Electoral TorresdeCotillas Jueves 0711 Ca...</td>\n",
              "      <td>2019</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Para llevarse tan mal se entienden muy bien. P...</td>\n",
              "      <td>PSOE</td>\n",
              "      <td>GOV</td>\n",
              "      <td>@sanchezcastejon</td>\n",
              "      <td>2018-03-26</td>\n",
              "      <td>llevarse tan mal entienden bien PP Cs vuelven ...</td>\n",
              "      <td>2018</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>#ComienzaElCambio en Andalucía tras cuatro déc...</td>\n",
              "      <td>PP</td>\n",
              "      <td>OPO</td>\n",
              "      <td>@pablocasado_</td>\n",
              "      <td>2019-01-16</td>\n",
              "      <td>ComienzaElCambio Andalucía tras cuatro décadas...</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   numero                                              tweet  ...   año mes\n",
              "0       0  Acabo de sumarme a la #HoradelPlaneta con @WWF...  ...  2017   3\n",
              "1       1  \"Nos hemos quedado huérfanos de una fiscalía q...  ...  2016   3\n",
              "2       2   Acto Electoral en las #TorresdeCotillas Jueve...  ...  2019  11\n",
              "3       3  Para llevarse tan mal se entienden muy bien. P...  ...  2018   3\n",
              "4       4  #ComienzaElCambio en Andalucía tras cuatro déc...  ...  2019   1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 342
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jJOPfscAKQr"
      },
      "source": [
        "#guardamos la version simplificada del df\n",
        "\n",
        "import csv\n",
        "with open('/content/gdrive/My Drive/proyecto_machine_learning/tweets_definitivo_hasta_2016_version_arreglada.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"numero\", \"tweet\", \"nombre\", \"posicion\", \"cuenta\", \"fecha\",\"año\",\"mes\",\"tweet_limpios\"])\n",
        "    for i in range(len(df)):\n",
        "        writer.writerow([i, df['tweet'][i],df['nombre'][i],df['posicion'][i],df['cuenta'][i],df['fecha'][i],df['año'][i],df['mes'][i],df['tweet_limpios'][i]])\n",
        "#df.to_csv('data_set_arreglado_y_procesado.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnAiNxL9G2Kz"
      },
      "source": [
        "## Simplificacion version 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOxpyL5aG1dV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "00389a17-6b66-4ad6-ead3-e7909a6fd17e"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/gdrive/My Drive/proyecto_machine_learning/tweets_definitivo_hasta_2016_version_sin_nada_reducido.csv')\n",
        "df.head(5) # for showing a snapshot of the dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>numero</th>\n",
              "      <th>tweet</th>\n",
              "      <th>nombre</th>\n",
              "      <th>posicion</th>\n",
              "      <th>cuenta</th>\n",
              "      <th>fecha</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Acabo de sumarme a la #HoradelPlaneta con @WWF...</td>\n",
              "      <td>PSOE</td>\n",
              "      <td>GOV</td>\n",
              "      <td>@JoseantonioJun</td>\n",
              "      <td>2017-03-22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>\"Nos hemos quedado huérfanos de una fiscalía q...</td>\n",
              "      <td>POD</td>\n",
              "      <td>GOV</td>\n",
              "      <td>@anamarcelloana</td>\n",
              "      <td>2016-03-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Acto Electoral en las #TorresdeCotillas Jueve...</td>\n",
              "      <td>VOX</td>\n",
              "      <td>OPO</td>\n",
              "      <td>@Vox_Molina</td>\n",
              "      <td>2019-11-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Para llevarse tan mal se entienden muy bien. P...</td>\n",
              "      <td>PSOE</td>\n",
              "      <td>GOV</td>\n",
              "      <td>@sanchezcastejon</td>\n",
              "      <td>2018-03-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>#ComienzaElCambio en Andalucía tras cuatro déc...</td>\n",
              "      <td>PP</td>\n",
              "      <td>OPO</td>\n",
              "      <td>@pablocasado_</td>\n",
              "      <td>2019-01-16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   numero  ...       fecha\n",
              "0       0  ...  2017-03-22\n",
              "1       1  ...  2016-03-04\n",
              "2       2  ...  2019-11-05\n",
              "3       3  ...  2018-03-26\n",
              "4       4  ...  2019-01-16\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ISK_NevQ6Ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "3fec1faf-abb2-48ad-96d5-3ac38576fc5c"
      },
      "source": [
        "# Importing necessary libraries\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from IPython.display import clear_output\n",
        "lemmatiser = WordNetLemmatizer()\n",
        "\n",
        "import tqdm\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "#limpiamos emoticonos y cosas varias que no son utiles para nuestro analisis\n",
        "\n",
        "def remove_pattern(input_txt, pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i, '', input_txt)        \n",
        "    return input_txt\n",
        "\n",
        "def eliminaremotideunstring(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "def eliminaremoti(lista):\n",
        "    for i in range(len(lista)):\n",
        "        lista[i]=eliminaremotideunstring(lista[i])\n",
        "    return lista\n",
        "\n",
        "\n",
        "def clean_tweets(lst):\n",
        "    # eliminamos URLs (httpxxx)\n",
        "    lst = np.vectorize(remove_pattern)(lst, \"https?://[A-Za-z0-9./]*\")\n",
        "    # eliminamos caracteres, numeros, signos depuntuacion (exceptuando #, los hastags son utiles)\n",
        "    lst = np.core.defchararray.replace(lst, \"[^a-zA-Z#]\", \" \")\n",
        "    # Eliminamos los emojis\n",
        "    lst = eliminaremoti(lst)\n",
        "    # eliminamos (@xxx)\n",
        "    lst = np.vectorize(remove_pattern)(lst, \"[\\n]*\")\n",
        "    lst = np.vectorize(remove_pattern)(lst, \"[:]*\")\n",
        "    lst = np.vectorize(remove_pattern)(lst, \"@[\\w]*\")\n",
        "    \n",
        "    return lst\n",
        "\n",
        "\n",
        "# Defining a module for Text Processing\n",
        "def text_process(tex):\n",
        "    # 1. Eliminamos todas las cosas de marcacion: puntos, comas, etc\n",
        "    nopunct=[char for char in tex if char not in string.punctuation]\n",
        "    nopunct=''.join(nopunct)\n",
        "    \n",
        "    # 2. Lemmatisation: proceso de agrupacion de palabras segun la raiz, por ejemplo\n",
        "    #    studing y study en el fondo son la misma palabra\n",
        "    \n",
        "    a=''\n",
        "    i=0\n",
        "    for i in range(len(nopunct.split())):\n",
        "        b=lemmatiser.lemmatize(nopunct.split()[i], pos=\"v\")\n",
        "        a=a+b+' '\n",
        "        \n",
        "    # 3. Eliminamos todas las palabras cortas e inutiles: a, an, the... que no nos\n",
        "    #   dicen absolutamente nada de la forma de escribir del autor\n",
        "    return [word for word in a.split() if word.lower() not in stopwords.words('spanish')] #usamos español\n",
        "    #return [word for word in nopunct.split() if word.lower() not in stopwords.words('spanish')]\n",
        "\n",
        "\n",
        "\n",
        "df['tweet_limpios'] = clean_tweets(df['tweet'])\n",
        "df['tweet_limpios'] = df['tweet_limpios'].str.replace(\"[^a-zA-Z#áéíóúñ]\", \" \")\n",
        "df['tweet_limpios'] = df['tweet_limpios'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "print(df.head(5))\n",
        "\n",
        "\n",
        "for i in range(len(df['tweet_limpios'])):\n",
        "#for i in tqdm.tqdm(range(100)):\n",
        "    df['tweet_limpios'][i] = text_process(df['tweet_limpios'][i])\n",
        "    df['tweet_limpios'][i] = ' '.join(df['tweet_limpios'][i])\n",
        "    clear_output(wait=True)\n",
        "    print(i, \"/\", len(df['tweet_limpios']), sep=\"\")   \n",
        "    \n",
        "\n",
        "    # or other long operations"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "138202/138203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFHyGN-aQ6Bs"
      },
      "source": [
        "from dateutil.parser import parse\n",
        "#from datetime import datetime\n",
        "#from dateutil.parser import parse\n",
        "\n",
        "año = []\n",
        "mes = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    año.append(parse(df['fecha'][i]).year)\n",
        "    mes.append(parse(df['fecha'][i]).month)\n",
        "df['año'] = año\n",
        "df['mes'] = mes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9A6sPeiE7Dy"
      },
      "source": [
        "import csv\n",
        "with open('/content/gdrive/My Drive/proyecto_machine_learning/tweets_definitivo_hasta_2016_version_super_arreglada.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"numero\", \"tweet\", \"nombre\", \"posicion\", \"cuenta\", \"fecha\",\"año\",\"mes\",\"tweet_limpios\"])\n",
        "    for i in range(len(df)):\n",
        "        writer.writerow([i, df['tweet'][i],df['nombre'][i],df['posicion'][i],df['cuenta'][i],df['fecha'][i],df['año'][i],df['mes'][i],df['tweet_limpios'][i]])\n",
        "#df.to_csv('data_set_arreglado_y_procesado.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sdEUpflE7rK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtQrncV7dFCu"
      },
      "source": [
        "## Simplificación Versión 3 (sin Hastags, no utilizada)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR-OO-hyE7pN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e3eb40e9-deef-460b-d50d-a36bf94e84e6"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/gdrive/My Drive/proyecto_machine_learning/tweets_definitivo_hasta_2016_version_sin_nada_reducido.csv')\n",
        "df.head(5) # for showing a snapshot of the dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>numero</th>\n",
              "      <th>tweet</th>\n",
              "      <th>nombre</th>\n",
              "      <th>posicion</th>\n",
              "      <th>cuenta</th>\n",
              "      <th>fecha</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Acabo de sumarme a la #HoradelPlaneta con @WWF...</td>\n",
              "      <td>PSOE</td>\n",
              "      <td>GOV</td>\n",
              "      <td>@JoseantonioJun</td>\n",
              "      <td>2017-03-22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>\"Nos hemos quedado huérfanos de una fiscalía q...</td>\n",
              "      <td>POD</td>\n",
              "      <td>GOV</td>\n",
              "      <td>@anamarcelloana</td>\n",
              "      <td>2016-03-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Acto Electoral en las #TorresdeCotillas Jueve...</td>\n",
              "      <td>VOX</td>\n",
              "      <td>OPO</td>\n",
              "      <td>@Vox_Molina</td>\n",
              "      <td>2019-11-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Para llevarse tan mal se entienden muy bien. P...</td>\n",
              "      <td>PSOE</td>\n",
              "      <td>GOV</td>\n",
              "      <td>@sanchezcastejon</td>\n",
              "      <td>2018-03-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>#ComienzaElCambio en Andalucía tras cuatro déc...</td>\n",
              "      <td>PP</td>\n",
              "      <td>OPO</td>\n",
              "      <td>@pablocasado_</td>\n",
              "      <td>2019-01-16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   numero  ...       fecha\n",
              "0       0  ...  2017-03-22\n",
              "1       1  ...  2016-03-04\n",
              "2       2  ...  2019-11-05\n",
              "3       3  ...  2018-03-26\n",
              "4       4  ...  2019-01-16\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZhaBVrME7dr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "0260f53f-df01-4597-880e-d60b9b8777ef"
      },
      "source": [
        "# Importing necessary libraries\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from IPython.display import clear_output\n",
        "lemmatiser = WordNetLemmatizer()\n",
        "\n",
        "import tqdm\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "def remove_pattern(input_txt, pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i, '', input_txt)        \n",
        "    return input_txt\n",
        "\n",
        "def eliminaremotideunstring(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "def eliminaremoti(lista):\n",
        "    for i in range(len(lista)):\n",
        "        lista[i]=eliminaremotideunstring(lista[i])\n",
        "    return lista\n",
        "\n",
        "\n",
        "def clean_tweets(lst):\n",
        "    # remove twitter Return handles (RT @xxx:)\n",
        "    lst = np.vectorize(remove_pattern)(lst, \"RT @[\\w]*:\")\n",
        "    # remove twitter handles (@xxx)\n",
        "    lst = np.vectorize(remove_pattern)(lst, \"@[\\w]*\")\n",
        "    # remove twitter hastags (@xxx)\n",
        "    lst = np.vectorize(remove_pattern)(lst, \"#[\\w]*\")\n",
        "    # remove URL links (httpxxx)\n",
        "    lst = np.vectorize(remove_pattern)(lst, \"https?://[A-Za-z0-9./]*\")\n",
        "    # remove special characters, numbers, punctuations (except for #)\n",
        "    lst = np.core.defchararray.replace(lst, \"[^a-zA-Z#]\", \" \")\n",
        "    # Eliminamos los emojis\n",
        "    lst = eliminaremoti(lst)\n",
        "    # remove twitter handles (@xxx)\n",
        "    lst = np.vectorize(remove_pattern)(lst, \"[\\n]*\")\n",
        "    lst = np.vectorize(remove_pattern)(lst, \"[:]*\")\n",
        "    \n",
        "    return lst\n",
        "\n",
        "\n",
        "# Defining a module for Text Processing\n",
        "def text_process(tex):\n",
        "    # 1. Eliminamos todas las cosas de marcacion, rollo puntos, comas, etc\n",
        "    nopunct=[char for char in tex if char not in string.punctuation]\n",
        "    nopunct=''.join(nopunct)\n",
        "    \n",
        "    # 2. Lemmatisation: proceso de agrupacion de palabras segun la raiz, por ejemplo\n",
        "    #    studing y study en el fondo son la misma palabra\n",
        "    \n",
        "    a=''\n",
        "    i=0\n",
        "    for i in range(len(nopunct.split())):\n",
        "        b=lemmatiser.lemmatize(nopunct.split()[i], pos=\"v\")\n",
        "        a=a+b+' '\n",
        "        \n",
        "    # 3. Eliminamos todas las palabras cortas e inutiles, rollo a, an, the... que no nos\n",
        "    #   dicen absolutamente nada de la forma de escribir del autor\n",
        "    return [word for word in a.split() if word.lower() not in stopwords.words('spanish')]\n",
        "    #return [word for word in nopunct.split() if word.lower() not in stopwords.words('spanish')]\n",
        "\n",
        "\n",
        "\n",
        "df['tweet_limpios'] = clean_tweets(df['tweet'])\n",
        "df['tweet_limpios'] = df['tweet_limpios'].str.replace(\"[^a-zA-Z#áéíóúñ]\", \" \")\n",
        "df['tweet_limpios'] = df['tweet_limpios'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "print(df.head(5))\n",
        "\n",
        "\n",
        "for i in range(len(df['tweet_limpios'])):\n",
        "#for i in tqdm.tqdm(range(100)):\n",
        "    df['tweet_limpios'][i] = text_process(df['tweet_limpios'][i])\n",
        "    df['tweet_limpios'][i] = ' '.join(df['tweet_limpios'][i])\n",
        "    clear_output(wait=True)\n",
        "    print(i, \"/\", len(df['tweet_limpios']), sep=\"\")   \n",
        "    \n",
        "\n",
        "    # or other long operations"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "138202/138203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUPSC0r6E7bj"
      },
      "source": [
        "from dateutil.parser import parse\n",
        "#from datetime import datetime\n",
        "#from dateutil.parser import parse\n",
        "\n",
        "año = []\n",
        "mes = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    año.append(parse(df['fecha'][i]).year)\n",
        "    mes.append(parse(df['fecha'][i]).month)\n",
        "df['año'] = año\n",
        "df['mes'] = mes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4flWfjAE7XF"
      },
      "source": [
        "import csv\n",
        "with open('/content/gdrive/My Drive/proyecto_machine_learning/tweets_definitivo_hasta_2016_version_super_arreglada_sin_hashtags.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"numero\", \"tweet\", \"nombre\", \"posicion\", \"cuenta\", \"fecha\",\"año\",\"mes\",\"tweet_limpios\"])\n",
        "    for i in range(len(df)):\n",
        "        writer.writerow([i, df['tweet'][i],df['nombre'][i],df['posicion'][i],df['cuenta'][i],df['fecha'][i],df['año'][i],df['mes'][i],df['tweet_limpios'][i]])\n",
        "#df.to_csv('data_set_arreglado_y_procesado.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4BgkRYLE7Uu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFQ7ayN1dkgS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyMyQSyWdkzA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtL2wEG6dkxE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmiGerSCdkvK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKyhfglvyIG5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "105fe516-f207-4eb1-fb80-9e480ae0e3b9"
      },
      "source": [
        "print(len(lista_completa_mezclada[61][0]))\n",
        "print(lista_completa_mezclada[92][0])\n",
        "print(TextBlob(str(np.core.defchararray.replace(lista_completa_mezclada[91][0], \"#\", \" \"))).translate(to='en'))\n",
        "print(TextBlob(str(np.core.defchararray.replace(lista_completa_mezclada[92][0], \"#\", \" \"))).detect_language())\n",
        "print(TextBlob('Miguel').detect_language())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "150\n",
            "Vox: how to understand the peculiarities of Spain’s hard-right movement via @TC_Africa\n",
            "In this electoral tacticism, Sánchez has not hesitated to use public institutions and organizations to his advantage either. A campaign that Ferraz instrumentalizes by mimicking it with La Moncloa even in its symbols.\n",
            "en\n",
            "es\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR856U2iyIUi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "3397517e-872b-42cb-e086-3abc42a60617"
      },
      "source": [
        "def traductor(frase):\n",
        "  frase_ingles = TextBlob(frase).translate(to='en')\n",
        "  return frase_ingles\n",
        "\n",
        "tweets_en_ingles = []\n",
        "for i in range(len(lista_completa_mezclada)):\n",
        "  if len(lista_completa_mezclada[i][0])==0: \n",
        "    tweets_en_ingles.append(lista_completa_mezclada[i][0])\n",
        "    continue\n",
        "  if TextBlob(str(np.core.defchararray.replace(lista_completa_mezclada[i][0], \"#\", \" \"))).detect_language() == 'en': \n",
        "    tweets_en_ingles.append(lista_completa_mezclada[i][0])\n",
        "    continue\n",
        "  tweets_en_ingles.append(TextBlob(str(np.core.defchararray.replace(lista_completa_mezclada[i][0], \"#\", \" \"))).translate(to='en'))\n",
        "  clear_output(wait=True)\n",
        "  print(i, \"/\", len(lista_completa_mezclada), sep=\"\") \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-330-ffdb4ad87357>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtweets_en_ingles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlista_completa_mezclada\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefchararray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlista_completa_mezclada\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"#\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_language\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'en'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtweets_en_ingles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlista_completa_mezclada\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/textblob/blob.py\u001b[0m in \u001b[0;36mdetect_language\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \"\"\"\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/textblob/translate.py\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(self, source, host, type_)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"q\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mu'{url}&sl=auto&tk={tk}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_calculate_tk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/textblob/translate.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, url, host, type_, data)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_proxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 642\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mhttp_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_302\u001b[0;34m(self, req, fp, code, msg, headers)\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0mhttp_error_301\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_error_303\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_error_307\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_error_302\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 642\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 429: Too Many Requests"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyY1CYImyISY"
      },
      "source": [
        "a=traductor(lista_completa_mezclada[0][0])\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECNYE3Se2c15"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdsAmqwL2dG9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty1y_Rte2dE4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8k6RIy7NVU-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRuKWaw2OTE-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "94b2c2ef-4f85-4db2-db47-220b17f6552c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17\n",
            "105\n",
            "105\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['En el #DiaMundialDelMedioAmbiente debemos reflexionar como sociedad sobre nuestra relación con el planeta y lo que debemos hacer para preservarlo para las próximas generaciones.  ',\n",
              " 'POD',\n",
              " 'GOV',\n",
              " '@PabloIglesias',\n",
              " '2020-06-05']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cdStklTQHbo"
      },
      "source": [
        "import csv\n",
        "with open('/content/gdrive/My Drive/proyecto_machine_learning/tweets_definitivo_hasta_2016.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"numero\", \"tweet\", \"nombre\", \"posicion\", \"cuenta\", \"fecha\"])\n",
        "    for i in range(len(lista_completa_mezclada)):\n",
        "        writer.writerow([i, lista_completa_mezclada[i][0], lista_completa_mezclada[i][1], lista_completa_mezclada[i][2],lista_completa_mezclada[i][3],lista_completa_mezclada[i][4]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afoletfoScpY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "ac2867a1-67c4-41e0-cba6-8ee87606f7ab"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/gdrive/My Drive/proyecto_machine_learning/tweets_definitivo_hasta_2016.csv')\n",
        "df.head(5) # for showing a snapshot of the dataset\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>numero</th>\n",
              "      <th>tweet</th>\n",
              "      <th>nombre</th>\n",
              "      <th>posicion</th>\n",
              "      <th>cuenta</th>\n",
              "      <th>fecha</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Qué gran labor a pie de calle ha realizado si...</td>\n",
              "      <td>PP</td>\n",
              "      <td>OPO</td>\n",
              "      <td>@abeltran_ana</td>\n",
              "      <td>2020-06-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Hasta 10.000 menús al día han llegado a salir ...</td>\n",
              "      <td>PP</td>\n",
              "      <td>OPO</td>\n",
              "      <td>@AlmeidaPP_</td>\n",
              "      <td>2020-06-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>De guardia civil a ministro por qué tiene uste...</td>\n",
              "      <td>CIU</td>\n",
              "      <td>OPO</td>\n",
              "      <td>@joanmesquida62</td>\n",
              "      <td>2020-06-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Hoy y todos los días, desde la  luchamos contr...</td>\n",
              "      <td>CIU</td>\n",
              "      <td>OPO</td>\n",
              "      <td>@ignacioaguado</td>\n",
              "      <td>2020-06-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>En el #DiaMundialDelMedioAmbiente debemos refl...</td>\n",
              "      <td>POD</td>\n",
              "      <td>GOV</td>\n",
              "      <td>@PabloIglesias</td>\n",
              "      <td>2020-06-05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   numero  ...       fecha\n",
              "0       0  ...  2020-06-05\n",
              "1       1  ...  2020-06-05\n",
              "2       2  ...  2020-06-05\n",
              "3       3  ...  2020-06-05\n",
              "4       4  ...  2020-06-05\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTkvZ6RQSupQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3g29ha6S8-p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZhz9fOnTIWp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLMVgVy6TVj0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}